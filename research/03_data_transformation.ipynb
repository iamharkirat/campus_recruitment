{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/harkiratchahal/Desktop/Coding/Tutorials/mlops_2/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/harkiratchahal/Desktop/Coding/Tutorials/mlops_2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlProject import logger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def handle_null_values(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle null values in the data.\"\"\"\n",
    "        logger.info(\"Handling Null Values...\")\n",
    "        return data.fillna(0, inplace=True)\n",
    "    \n",
    "    def drop_columns(self, data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        \"\"\"Drop specified columns.\"\"\"\n",
    "        logger.info(f\"Dropping Columns: {', '.join(columns)}\")\n",
    "        return data.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "    def handle_outliers(self, data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "        \"\"\"Handle outliers in a specified column.\"\"\"\n",
    "        logger.info(f\"Handling Outliers in Column: {column}\")\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filter = (data[column] >= Q1 - 1.5 * IQR) & (data[column] <= Q3 + 1.5 * IQR)\n",
    "        return data.loc[filter]\n",
    "\n",
    "    def encode_labels(self, data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        \"\"\"Label encode specified columns.\"\"\"\n",
    "        logger.info(f\"Encoding Labels for Columns: {', '.join(columns)}\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        for col in columns:\n",
    "            data[col] = label_encoder.fit_transform(data[col])\n",
    "        return data\n",
    "    \n",
    "    def dummy_encode(self, data: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        \"\"\"Dummy encode specified columns.\"\"\"\n",
    "        logger.info(f\"Dummy Encoding Columns: {', '.join(columns)}\")\n",
    "        for col in columns:\n",
    "            dummies = pd.get_dummies(data[col], prefix=f'dummy_{col}', drop_first=True)\n",
    "            dummies = dummies.astype(int)\n",
    "            data = pd.concat([data, dummies], axis=1)\n",
    "            data = data.drop(columns=[col])\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def split_and_save(self, data: pd.DataFrame, test_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split data into train and test sets and save them as CSV files.\"\"\"\n",
    "        logger.info(f\"Splitting Data with Test Size: {test_size}\")\n",
    "        train, test = train_test_split(data, test_size=test_size)\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "        logger.info(\"Data has been split and saved.\")\n",
    "        return train, test\n",
    "\n",
    "    def perform_data_transformation(self):\n",
    "        \"\"\"Perform data transformations and split data into train and test.\"\"\"\n",
    "        logger.info(\"Starting Data Transformation Process...\")\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        self.handle_null_values(data)\n",
    "        self.drop_columns(data, ['sl_no', 'ssc_b', 'hsc_b'])\n",
    "        data = self.handle_outliers(data, 'hsc_p')\n",
    "        data = self.encode_labels(data, ['gender', 'workex', 'specialisation', 'status'])\n",
    "        data = self.dummy_encode(data, ['hsc_s', 'degree_t'])\n",
    "        train, test = self.split_and_save(data, test_size=0.25)\n",
    "        logger.info(\"Data Transformation Process Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-03 01:07:39,246: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2023-10-03 01:07:39,252: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-10-03 01:07:39,256: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-10-03 01:07:39,257: INFO: common: created directory at: artifacts]\n",
      "[2023-10-03 01:07:39,258: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-10-03 01:07:39,258: INFO: 537558814: Starting Data Transformation Process...]\n",
      "[2023-10-03 01:07:39,290: INFO: 537558814: Handling Null Values...]\n",
      "[2023-10-03 01:07:39,293: INFO: 537558814: Dropping Columns: sl_no, ssc_b, hsc_b]\n",
      "[2023-10-03 01:07:39,300: INFO: 537558814: Handling Outliers in Column: hsc_p]\n",
      "[2023-10-03 01:07:39,308: INFO: 537558814: Encoding Labels for Columns: gender, workex, specialisation, status]\n",
      "[2023-10-03 01:07:39,313: INFO: 537558814: Dummy Encoding Columns: hsc_s, degree_t]\n",
      "[2023-10-03 01:07:39,325: INFO: 537558814: Splitting Data with Test Size: 0.25]\n",
      "[2023-10-03 01:07:39,342: INFO: 537558814: Data has been split and saved.]\n",
      "[2023-10-03 01:07:39,343: INFO: 537558814: Data Transformation Process Completed.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.perform_data_transformation()\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>ssc_p</th>\n",
       "      <th>hsc_p</th>\n",
       "      <th>degree_p</th>\n",
       "      <th>workex</th>\n",
       "      <th>etest_p</th>\n",
       "      <th>specialisation</th>\n",
       "      <th>mba_p</th>\n",
       "      <th>status</th>\n",
       "      <th>salary</th>\n",
       "      <th>dummy_hsc_s_Commerce</th>\n",
       "      <th>dummy_hsc_s_Science</th>\n",
       "      <th>dummy_degree_t_Others</th>\n",
       "      <th>dummy_degree_t_Sci&amp;Tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1</td>\n",
       "      <td>65.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40.89</td>\n",
       "      <td>45.83</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2</td>\n",
       "      <td>1</td>\n",
       "      <td>65.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>74.00</td>\n",
       "      <td>62.00</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.99</td>\n",
       "      <td>1</td>\n",
       "      <td>268000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>52.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>68.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.59</td>\n",
       "      <td>1</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  ssc_p  hsc_p  degree_p  workex  etest_p  specialisation  mba_p  \\\n",
       "0       0  47.00  55.00      65.0       0     62.0               1  65.04   \n",
       "1       0  40.89  45.83      53.0       0     71.2               1  65.49   \n",
       "2       1  74.00  62.00      68.0       0     74.0               0  57.99   \n",
       "3       1  52.00  65.00      57.0       1     75.0               0  59.81   \n",
       "4       1  68.00  76.00      74.0       0     80.0               0  63.59   \n",
       "\n",
       "   status    salary  dummy_hsc_s_Commerce  dummy_hsc_s_Science  \\\n",
       "0       0       0.0                     0                    1   \n",
       "1       0       0.0                     1                    0   \n",
       "2       1  268000.0                     0                    1   \n",
       "3       0       0.0                     0                    0   \n",
       "4       1  360000.0                     1                    0   \n",
       "\n",
       "   dummy_degree_t_Others  dummy_degree_t_Sci&Tech  \n",
       "0                      0                        0  \n",
       "1                      0                        0  \n",
       "2                      0                        0  \n",
       "3                      1                        0  \n",
       "4                      0                        0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv('/Users/harkiratchahal/Desktop/Coding/Tutorials/mlops_2/artifacts/data_transformation/train.csv')\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                     0\n",
       "ssc_p                      0\n",
       "hsc_p                      0\n",
       "degree_p                   0\n",
       "workex                     0\n",
       "etest_p                    0\n",
       "specialisation             0\n",
       "mba_p                      0\n",
       "status                     0\n",
       "salary                     0\n",
       "dummy_hsc_s_Commerce       0\n",
       "dummy_hsc_s_Science        0\n",
       "dummy_degree_t_Others      0\n",
       "dummy_degree_t_Sci&Tech    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('mlops')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cef9e7b2bdb756505b6584b8cd6e67f58b1526fe5bee48dc10ccd8818f1be2db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
